В лабе по тасклетам обработчик тасклета нигде явно не вызывается, но дело в том что его выполнение планируется.

Все отложенные действия выполняются в системе как потоки, то есть на самом деле запускается поток. Этими потоками управляет kernel softirq_daemon. Набрать ps - ajx и посмотреть на этих демонов, в том числе на workers. Увидим что для каждого ядра есть соответсвтующий воркер. По разному запускаются потоки. Тасклет - один из типов софтирку. Один и тот же софтирку может выполняться параллельно. Для этого там необходимо строжайшим образом выполнять взаимоисключение. В тасклетах тоже может быть взаимоисключение.

Каким образом это может делаться? тасклеты не могут блокироваться, но в них могут использоваться спинлоки. И на тасклетах могут быть определены соответствующие спинлоки. 

~~~

static inline int tasklet_trylock(struct tasklet_struct *t)
{
	return !test_and_set_bit(TASKLET_STATE_RUN, &(t)->state);
}

static inline void tasklet_unblock(struct tasklet_struct *t)
{
	smp_mb_before_atomic(); //указывается что smp потому что многоядерная система, наши системы имеют smp архитектуру
	clear_bit(TASKLET_STATE_RUN, &(t)->state);
}

~~~

Флаг test_and_set проверяется потому что тасклет может выполняться только 1. Даже если в очереди запланирован этот тасклет много раз, он будет выполнен один раз. ...? 

~~~

static inline void tasklet_schedule(struct tasklet_struct *t)
{
	if (!test_and_set_bit(TASKLET_STATE_SCHED, &t->state))
	 __tasklet_schedule(t);
}


enum 
{
	TASKLET_STATE_SCHED,
	TASKLET_STATE_RUN  //только для SMP (на сегодняшний день это не актуально)
}

~~~

ДВА ОДИНАКОВЫХ ТАСКЛЕТА НЕ МОГУТ ВЫПОЛНЯТЬСЯ ОДНОВРЕМЕННО!

когда вызываем declare_tasklet то автоматически запускается поток. То есть поток ставится в очередь. В ps -ajx мы видим процессы. Ядро UNIX монолитное. Монолитное ядро состоит из функций, вы это видете в коде. Но при этом всегда говорится, что оно многопоточное.  ps -ajx это процессы.  У каждого есть идентификатор. Любов процесс запускается как потомок какого-то процесса, то есть системным вызовом fork. Сам системный вызов fork это очень низкоуровневое действие. И запущеный тасклет, и запущеная работа из очереди работ это поток, запланировать мы можем только поток. Планируются к выполнению процессы.



Обратить внимание, что на тасклетах определены свои спинлоки и мы их рассмотрели.

ОЧЕРЕДИ РАБОТ

Это так же отложенные действия. Это структуры и функции которые переписывались и безусловно они переписывались для того чтобы ещё более отличать их по действиям от тасклетов. Везде подчеркивается что тасклеты не могут блокироваться - вызывать слип, семафоры. Безусловно может оказаться такая ситуация когда разные тасклеты обращаются к одним и тем же данным. Нельзя гарантировать, что будет строго тасклет который выполняет какие-то действия и никаких пересечений с другими отложенными действиями у тасклета не будет. Очереди работ так же отложенные действия, но запускаются по другому. Тасклеты и софтирку запускаются ksoftirq_daemon, а очереди работ запускаются по другому.

Прежде всего, очередь работ может инициализироваться статически и динамически. Для статической декларации работы используюется declare_work. Но когда говорим о работах, мы всегда говорим о queue_work. Подчеркивается, что существует очередь работ в которую помещаются работы. Конечно struct work_struct меньше чем struct queue_struct. В очередь работ помещаются какие-то работы и в результате эти работы запускаются из этой очереди работ. И это все потоки.

Для того чтобы статически инициализировать work используется макрос DECLARE_WORK(name, void(*func)(void*)) 
//name - имя  структуры work_struct и void(*func)(void*)  указатель на функцию (обработчик работы)

Динамически можем инициализировать работу с помощью INIT_WORK.

INIT_WORK(struct work_struct *work, void (*func)(void), void *data); //выполняет инициализацию работы более тщательно и рекомендуется использовать если работы инициализируется первый раз

PREPARE_WORK(--//--); // если работа уже инициализирована и требуется изменить какие-то параметры, то рекомендуется вызывать prepare_work.

Возвращаясь к загружаемому модулю ядра, в ините мы вызываем request_irq, делаем так же как делали в лр с тасклетом - передаем номер линии ирку, handler, устанавливаем IRQF_SHARED. Если удалось это сделать, тогда мы создаем очередь. Здесь источники резко расходятся. 

~~~

static struct workqueue_struct *queue;
...init()
{
	...
	queue = create_workqueue("workqueu");
	if (!queue){
		return error	
	}
}
~~~

Можно прочитать, что create_workqueue это устаревшая функция и надо вызывать alloc_workqueue(...,...,...,...);


~~~

struct workqueue_struct *alloc_workqueue(const char *fmt, unsigned int flags, int max_active, ...);

~~~

Фактически получается что размещается очередь работ, фактически создаётся.

const char *fmt - формат печати для имени workqueue. 

unsigned int flags - флаги для очередей работ.

max_active -  ограничивает число задач которые могут выполняться одновременно в данной очереди работы ..?

Allocate an odered workqueue. An odered workqueue executes at most one work item at any given time in quened order.

Если очередь не привязана, а для привязанной очереди тем более будет выполняться одна работа в каждый момент времени. 

Посмотрим create_workqueue(name)

~~~
#define create_workqueue(name)
	alloc_workqueue("%s", _WQ_LEGACY|WQ_MEM_RECLAIM, 1, (name))
~~~

create_workqueue освобождает нас от необходимости указывать перечень параметров которые требует указывать функция alloc_workqueue. 

Теперь необходимо поместить работу в очередь. Работа помещается в очередь с функции queue_work.


~~~

int queue_work(struct workqueue_struct *wq, struct work_struct *w);
 ....?

int queue_work_on(int cpu, --//--)


 Определяет номер процессора на котором будет выполняться работа.

Кроме этих функций есть 2 дополнительные функции которые позволяют запланировать работу, не только ее запланировать но и отложить на неопределенное время.

int queue_dalayed_work(struct workqueue_struct *wq, struct delayed_work *dw, unsigned long delay);

struct delayed_work - выписать, посмотреть

int queue_dalay_work_on(int cpu)

Все функции ставят в очередь ядра? работу. ? Такой драйвер может быть написан как загружаемый модуль ядра. Линукс позволяет изменять функциональность, используя ? который будет принимать информацию например от драйвера клавиатуры. Это похоже на то как в Windows реализованы ? драйверы. (тут дописать, я уже не вывожу, а она летит).

В любой системе внешним устройством управляет функциональный драйвер. Как правило, драйвер поставляется вместе с внешним устройством. Сейчас такой драйвер можно скачать на сайте разработчика. Windows обычно сам содержит большой набор драйверов разных фирм(для принтерова например). В результате будет установлен драйвер который будет управлять работой внешнего устройства. Обычно внешним устройством управляют так называемые функциональные драйверы. У любого драйвера может быть 1 обработчик прерывания. 

? .........

Кроме очередей которые мы сами создаем, есть некая глоабльная очередь и в неё тоже можно поставить работу. Для этого используются другие функции. 

sheduler_work

int shedule_work(struct work_struct *w);

Обратите внимание, здесь в параметрах нет struct workqueue_struct, так как мы ее не создаём.

(в методе перечислены 4 функции)

Разница заключается в том, что в первом случае работа ставится в созданную нами очередь, а здесь работа ставится в глобальную очередь работ. Для того чтобы принудительно завершить или отменить работу из очереди используется функция flush_work.

int flush_work(struct work_struct *w);

Для того чтобы завершить глобальную очередь ?

int flush_scheduled_work(void);


завершение всей очереди

int flush_workqueue(struct workqueue_struct *wq);




В лр создать одну очередь struct worqueue_struct и две работы struct work_struct. Будет два обработчика, work1 и work2.Надо чтоб делали что-то отличающееся. Очереди вот эти надо продемонстировать что они могут блокироваться. Например work_handler который воспроизводит мелодии.
В exit надо вызвать функцию destroy_workqueue. Основное требование это 2 работы и проследить их создание в логе. По структуре struct_work вывести информацию об этой работе. Лучше через proc, но можно не через proc. И информацию о созданной очереди. 























